{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h1> <center> ENSF 519.01 Programming Fundamentals for Data Engineers </center></h1>\n",
    "<h2> <center> Lab 9 (exercise): Introduction to Classification and Regression </center></h2>\n",
    "<h3> <center> This Lab will not be graded.  </center></h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignemnt focuses on applying basic classfication and regression techniques in the context of sofware quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A. Defect Prediction</h2>\n",
    "<br>\n",
    "\n",
    "Software quality assurance (SQA) techniques (e.g., testing, code review, etc.) are among the major tasks in software development that try to eliminate software defects, as much as possible, prior to deployment. However, they are also expensive and time consuming. Therefore, it is very beneficial to narrow the scope of SQA to the parts of the software (e.g., files, classes, or even methods) that are defective. Obviously, before applying SQA, the defective parts are unknown, therefore, “software defect prediction” techniques try to use machine learning to predict which parts of the software (e.g., which files) are more likely to be defective. Thus the SQA effort can be proportionally allocated to them (more SQA resources for parts that are predicted to be defective). \n",
    "\n",
    "In Parts A and B you will apply two supervised machine learning techniques to a defect dataset from a software repository, which is belong to a NASA system. The dataset have collected a set of software related metrics from the history of the software, per module. They also recorded the existence of a defect per module. In other words, your feature set is the metrics array and the targets are the existence of defects. \n",
    "\n",
    "The goal is to build a model to predict the target based on the features from the historical data. \n",
    "\n",
    "To read more about the dataset and the features collected see:\n",
    "\n",
    "NASA dataset:http://openscience.us/repo/defect/mccabehalsted/jm1.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Binary Classification </h2>\n",
    "\n",
    "First, you should read the NASA data that is given to you as a .csv file ( NasaData.csv ). Use pandas read_csv function for this.\n",
    "\n",
    "After reading the data, build binary classification models with KNeighborsClassifier, from SKLearn. The model gets metrics as feature set and predicts either a defective (1) or not defective (0) label.\n",
    "\n",
    "Take 75% of data as training set and 25% of it as test set. To eliminate the randomness when splitting the data, you should run each classification technique 30 times with seeds from [1 to 30]. \n",
    "\n",
    "Using model_selection's train_test_split function, randomly select 1/4 of your dataset as training and 3/4 as testset. \n",
    "Calculate accuracies per technique and repeat this for a total of 30 random runs (every run will use a different random seed in train_test_split and return a separate accuracy value per model).\n",
    "\n",
    "Visualize the distribution of the accuracies for each model in a single box plot, where The X_axis is the  classification technique that you've applied and the Y_axis is the accuracies.\n",
    "\n",
    "\n",
    "*** Note1 you must use pandas for both reading from CSV and visualizing boxplots ***\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Part A.1. Solution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B. KNN complexity tuning </h2>\n",
    "In this part, we want to tune the value k in kNN for our NASA dataset. To do so, you should find a sweet spot that the model is neither overfitted nor underfitted. \n",
    "Here again take the NASA dataset and apply the model_selection's train_test_split with 75% training and 25% test data, but with a fix random_state=42.  \n",
    "\n",
    "Then build a K-Nearest-Neighbors model using k=1,3,5,..,49. Finally, plot the accuracy of your models on the training dataset and the testing dataset, using two lines in one plot.  \n",
    "\n",
    "Using this plot identify what the best value is for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B.1. Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C. Regression </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a new data set which is related to the performance of several CPUs. These CPUs are of different specifications, and you have the estimated relative performance(ERP) metric per CPU, in this data set.\n",
    "\n",
    "columns of data set are as follows:\n",
    "\n",
    "    MYCT: machine cycle time in nanoseconds (integer) \n",
    "    MMIN: minimum main memory in kilobytes (integer) \n",
    "    MMAX: maximum main memory in kilobytes (integer) \n",
    "    CACH: cache memory in kilobytes (integer) \n",
    "    CHMIN: minimum channels in units (integer) \n",
    "    CHMAX: maximum channels in units (integer) \n",
    "    PRP: published relative performance (integer) \n",
    "    ERP: estimated relative performance from the original article (integer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data that is given to you as a CSV file (\"CPU_Performance.csv\") and take 75% of it as training set and 25% of it as test set with random_state=42.\n",
    "\n",
    "Use default KNeighborsRegressor, to predict ERP using the other columns as features.\n",
    "\n",
    "To see how good you can predict on new CPUs performance, print the score of the model on training set and test set data.\n",
    "\n",
    "Use the default setup for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part D.1. Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
